{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVAZt6s-Fu41"
   },
   "source": [
    "# AI6126 ACV Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1601175331072,
     "user": {
      "displayName": "Jia Hui Ong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKvHQmYQfzydylrU8HXZxYxJP3L6kGAQ94P-sS=s64",
      "userId": "05957301376516334331"
     },
     "user_tz": -480
    },
    "id": "jzjCg15BeR43"
   },
   "outputs": [],
   "source": [
    "nb_ver = 1.3\n",
    "title = f'ai6126-p1-train-v{nb_ver}'\n",
    "print(title)\n",
    "comments = \"TEST\"\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versioning & References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c2s81g4d5PE"
   },
   "source": [
    "### Changelogs\n",
    "+ V0.1 - Setup codes to download and unzip celeba to gDrive\n",
    "+ V0.2 - Added training loop \n",
    "+ V0.3 - Added seeding + save/ load checkpoint\n",
    "+ V0.4 - Added time taken + save output\n",
    "+ V0.5 - Added RandomErasing to transforms\n",
    "+ V0.6 - Added get_criterion (FocalLoss) \n",
    "+ V0.7 - Added FaceAttrMobileNetV2 & FaceAttrResNeXt\n",
    "+ V0.8 - Added Albumentations\n",
    "+ V0.9 - Updated Optimizer (SGD, AdamW works well)\n",
    "+ V0.91 - Added ModelTimer() + Added more augmentations\n",
    "+ V1.0 - Added ReduceLROnPlateau Scheduler\n",
    "+ V1.1 - Updated Augmentations to more closely follow Tricks paper + Added OneCycleLR Scheduler + No bias decay\n",
    "+ V1.2 - Added Early Stopping\n",
    "+ V1.3 - Code Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo:\n",
    "+ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "+ [Face Attribute Prediction on CelebA benchmark with PyTorch Implementation](https://github.com/d-li14/face-attribute-prediction)\n",
    "+ [PyTorch Transfer Learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "+ [Albumentations](https://albumentations.ai/)\n",
    "+ [Focal Loss](https://github.com/kornia/kornia/blob/master/kornia/losses/focal.py)\n",
    "+ [Bag of Tricks](https://arxiv.org/abs/1812.01187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "# conda install matplotlib\n",
    "# conda install pandas\n",
    "# conda install tqdm\n",
    "# conda install -c conda-forge jupyterlab\n",
    "# conda install -c conda-forge tensorboard\n",
    "# conda install -c conda-forge protobuf # for tensorboard\n",
    "# conda install nb_conda_kernels # auto add kernels\n",
    "\n",
    "# conda install -c conda-forge imgaug\n",
    "# conda install albumentations -c conda-forge\n",
    "# conda install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmjjrIheF0u5"
   },
   "source": [
    "## Setup/ Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 4455,
     "status": "error",
     "timestamp": 1601175336681,
     "user": {
      "displayName": "Jia Hui Ong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKvHQmYQfzydylrU8HXZxYxJP3L6kGAQ94P-sS=s64",
      "userId": "05957301376516334331"
     },
     "user_tz": -480
    },
    "id": "7mTWwwivDNy3",
    "outputId": "9f87be63-e615-42bf-b306-8a2ec7e2458d"
   },
   "outputs": [],
   "source": [
    "# you can choose to mount your Google Drive (optional)\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    file_name = f'ai6126-project1-colab-v{nb_ver}.ipynb'\n",
    "    print(file_name)\n",
    "    import subprocess\n",
    "    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "    print(path_to_file)\n",
    "    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset (JUPYTER ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download celeba dataset: False\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "local_download_path = '../data/celeba/img_align_celeba'\n",
    "download_dataset = True\n",
    "if os.path.exists(local_download_path):\n",
    "    images = glob.glob(local_download_path + '/*.jpg')\n",
    "    if len(images) == 202599:\n",
    "        download_dataset = False\n",
    "print(f\"download celeba dataset: {download_dataset}\")\n",
    "\n",
    "if download_dataset:\n",
    "    # create dataset root and enter it\n",
    "    !mkdir -p data/celeba\n",
    "    %cd data/celeba\n",
    "\n",
    "    # we have prepared a backup of `img_align_celeba.zip` of Celeb-A dataset in the Dropbox\n",
    "    # download it directly, or manually download the original file from Google Drive above\n",
    "    !wget https://www.dropbox.com/s/8kzo40fqx7nodat/img_align_celeba.zip\n",
    "\n",
    "    # unzip the downloaded file\n",
    "    !unzip -qq img_align_celeba.zip\n",
    "    !rm -f img_align_celeba.zip\n",
    "\n",
    "    # change the directory back to the root\n",
    "    %cd ../..\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tEkTpN_qDN5u"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from distutils.dir_util import copy_tree #for recursive filecopying\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config\n",
    "from celeba_dataset import CelebaDataset\n",
    "import models\n",
    "import losses\n",
    "from utils import Logger, AverageMeter, Bar, ModelTimer, savefig, adjust_learning_rate, accuracy, print_attribute_acc, create_dir_ifne, add_weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DCjPfxzUDN3Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0 True\n",
      "cuda:0\n",
      "disable_tqdm: False\n"
     ]
    }
   ],
   "source": [
    "# check PyTorch version and cuda status\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# define device\n",
    "device = torch.device(\"cuda:\"+config.gpu_id if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "ISJUPYTER = False\n",
    "if 'ipykernel' in sys.modules:\n",
    "    ISJUPYTER = True\n",
    "    # set the backend of matplotlib to the 'inline' backend\n",
    "    %matplotlib inline\n",
    "    config.disable_tqdm = False\n",
    "    \n",
    "print(f\"disable_tqdm: {config.disable_tqdm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 4483,
     "status": "ok",
     "timestamp": 1601174828368,
     "user": {
      "displayName": "Jia Hui Ong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKvHQmYQfzydylrU8HXZxYxJP3L6kGAQ94P-sS=s64",
      "userId": "05957301376516334331"
     },
     "user_tz": -480
    },
    "id": "9Te1EuMyDN7-",
    "outputId": "e1a9321b-0c11-476e-dea5-57b3fa33f732"
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "def seed_everything(seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000) # create random seed\n",
    "        print(f'random seed used: {seed}')\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=config.manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7867,
     "status": "ok",
     "timestamp": 1601174831763,
     "user": {
      "displayName": "Jia Hui Ong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKvHQmYQfzydylrU8HXZxYxJP3L6kGAQ94P-sS=s64",
      "userId": "05957301376516334331"
     },
     "user_tz": -480
    },
    "id": "UeTJbZQ5b_ea",
    "outputId": "de5f0be8-65fe-4648-e719-ff92f2642235",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "### Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation and testing\n",
    "def load_dataloaders(print_info=True, albu_transforms = True):\n",
    "    if config.evaluate:\n",
    "        phases = ['test']\n",
    "    else:\n",
    "        phases = ['train', 'val']\n",
    "\n",
    "    attribute_names = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', \n",
    "                       'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', \n",
    "                       'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
    "                       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', \n",
    "                       'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
    "                       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', \n",
    "                       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
    "    \n",
    "    attributes_list = {\n",
    "        'train': config.TRAIN_ATTRIBUTE_LIST,\n",
    "        'val': config.VAL_ATTRIBUTE_LIST,\n",
    "        'test': config.TEST_ATTRIBUTE_LIST\n",
    "    }\n",
    "\n",
    "    batch_sizes = {\n",
    "        'train': config.train_batch,\n",
    "        'val': config.test_batch,\n",
    "        'test': config.test_batch\n",
    "    }\n",
    "\n",
    "    if not albu_transforms:\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.CenterCrop(148), #new\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=10), #new\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "                transforms.RandomErasing()\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(178), #new\n",
    "                transforms.CenterCrop(148),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(178), #new\n",
    "                transforms.CenterCrop(148),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        }\n",
    "    else:\n",
    "        normalize_A = A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                  std=(0.229, 0.224, 0.225))\n",
    "        data_transforms = {\n",
    "            'train': A.Compose([\n",
    "                #A.RandomResizedCrop(148, 148), # cuts out too much attributes, use centercrop instead\n",
    "                A.CenterCrop(height=148, width=148),\n",
    "                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, \n",
    "                                 rotate_limit=15, p=0.5), # AFFACT https://arxiv.org/pdf/1611.06158.pdf\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=14, sat_shift_limit=14, val_shift_limit=14, p=0.5),\n",
    "                A.FancyPCA(alpha=0.1, p=0.5), #http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.GaussNoise(var_limit=10.0, p=0.5), \n",
    "                #A.GaussianBlur(p=0.1), # AFFACT https://arxiv.org/pdf/1611.06158.pdf\n",
    "                A.CoarseDropout(max_holes=1, max_height=74, max_width=74, \n",
    "                               min_height=49, min_width=49, fill_value=0, p=0.2), #https://arxiv.org/pdf/1708.04896.pdf\n",
    "                normalize_A,\n",
    "                ToTensorV2(),\n",
    "                \n",
    "            ]),\n",
    "            'val': A.Compose([\n",
    "                #Rescale an image so that minimum side is equal to max_size 178 (shortest edge of Celeba)\n",
    "                A.SmallestMaxSize(max_size=178), \n",
    "                A.CenterCrop(height=148, width=148),\n",
    "                normalize_A,\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "            'test': A.Compose([\n",
    "                A.SmallestMaxSize(max_size=178),\n",
    "                A.CenterCrop(height=148, width=148),\n",
    "                normalize_A,\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    image_datasets = {x: CelebaDataset(config.IMG_DIR, attributes_list[x], \n",
    "                                       data_transforms[x]) \n",
    "                      for x in phases}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                  batch_size=batch_sizes[x],\n",
    "                                                  pin_memory=True, shuffle=(x == 'train'), \n",
    "                                                  num_workers=config.dl_workers) \n",
    "                   for x in phases}\n",
    "    if print_info:\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in phases}\n",
    "        print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "        \n",
    "    if config.evaluate:\n",
    "        class_names = image_datasets['test'].targets\n",
    "    else:\n",
    "        class_names = image_datasets['train'].targets\n",
    "    print(f\"Class Labels: {len(class_names[0])}\")\n",
    "    assert len(attribute_names) == len(class_names[0])\n",
    "    return dataloaders, attribute_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtfDq7Z2F6Nh",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "### Model Architecture Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['FaceAttrMobileNetV2', 'FaceAttrResNeXt', 'FaceAttrResNet']\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if callable(models.__dict__[name])) # and name.islower() and not name.startswith(\"__\"))\n",
    "print(f\"Available Models: {model_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'FaceAttrResNet'\n"
     ]
    }
   ],
   "source": [
    "def create_model(arch, layers, device):\n",
    "    print(\"=> creating model '{}'\".format(config.arch))\n",
    "    if arch.startswith('FaceAttrResNet'):\n",
    "        model = models.__dict__[arch](resnet_layers = layers)\n",
    "    elif arch.startswith('FaceAttrResNeXt'):\n",
    "        model = models.__dict__[arch](resnet_layers = layers)\n",
    "    elif arch.startswith('FaceAttrMobileNetV2'):\n",
    "        model = models.__dict__[arch]()\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "model = create_model(config.arch, config.pt_layers, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion & Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_criterion():\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    if config.criterion == 'BinaryFocalLoss':\n",
    "        criterion = losses.BinaryFocalLoss().to(device)\n",
    "    elif config.criterion == 'FocalLoss':\n",
    "        criterion = losses.FocalLoss(alpha=0.25, gamma=5, reduction='mean').to(device) #alpha=0.25, gamma =5\n",
    "    return criterion\n",
    "\n",
    "criterion = get_criterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name=config.optimizer, no_bias_bn_decay=config.no_bias_bn_decay):\n",
    "    weight_decay = config.weight_decay\n",
    "    if no_bias_bn_decay: #bag of tricks paper\n",
    "        parameters = add_weight_decay(model, weight_decay)\n",
    "        weight_decay = 0.\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "    \n",
    "    optimizer = None\n",
    "    if opt_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(parameters, config.lr,\n",
    "                                momentum=config.momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "    elif opt_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(parameters, config.lr,\n",
    "                            weight_decay=weight_decay)\n",
    "    elif opt_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(parameters, config.lr,\n",
    "                            weight_decay=weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, steps_per_epoch, epochs):\n",
    "    scheduler = None # Manual\n",
    "    if config.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                               factor=0.05,\n",
    "                                                               patience=config.patience)\n",
    "    elif config.scheduler == 'OneCycleLR': \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, epochs=epochs,\n",
    "                                                        steps_per_epoch=int(steps_per_epoch), \n",
    "                                                        anneal_strategy='cos') #https://arxiv.org/pdf/1708.07120.pdf\n",
    "    return scheduler    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Checkpoint if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resume_checkpoint(model, optimizer, ckp_logger_fname, ckp_resume=None):\n",
    "#     if not os.path.isdir(config.CHECKPOINT_DIR):\n",
    "#         try: \n",
    "#             os.makedirs(config.CHECKPOINT_DIR)\n",
    "#         except OSError:\n",
    "#             raise\n",
    "            \n",
    "#     start_epoch = 0\n",
    "#     best_prec1 = 0\n",
    "#     lr = config.lr\n",
    "\n",
    "#     if ckp_resume and os.path.isfile(ckp_resume): \n",
    "#         print(f\"=> resuming model: {ckp_resume}\")\n",
    "#         optimizer, start_epoch, best_prec1, lr, total_train_time = model.load_ckp(optimizer, ckp_resume)\n",
    "#         model_timer = ModelTimer(total_train_time)\n",
    "#         if config.scheduler == 'Manual':\n",
    "#             lr = config.lr # self adjust from original\n",
    "#         #config.checkpoint = os.path.dirname(ckp_resume)\n",
    "#         logger = Logger(ckp_logger_fname, title=model.name, resume=True)\n",
    "#     else:\n",
    "#         model_timer = ModelTimer()\n",
    "#         logger = Logger(ckp_logger_fname, title=model.name)\n",
    "#         logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])\n",
    "        \n",
    "        \n",
    "#     return best_prec1, model_timer, lr, start_epoch, logger\n",
    "\n",
    "# # best_prec1, mt, lr, start_epoch, logger = resume_checkpoint(model, optimizer, config.ckp_logger_fname, config.ckp_resume)\n",
    "# # print(f\"=> Model trained time: {mt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_checkpoint(device, ckp_logger_fname, ckp_resume=None):\n",
    "    if not ckp_logger_fname:\n",
    "        print(\"[W] Logger path not found.\")\n",
    "        raise\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_prec1 = 0\n",
    "    lr = config.lr\n",
    "    \n",
    "    if ckp_resume and os.path.isfile(ckp_resume): \n",
    "        print(f\"=> resuming checkpoint: {ckp_resume}\")\n",
    "        checkpoint = torch.load(ckp_resume)\n",
    "        \n",
    "        try:\n",
    "            total_time = checkpoint['total_time']\n",
    "            model_timer = ModelTimer(total_time)\n",
    "            print(f\"=> model trained time: {model_timer}\")\n",
    "        except:\n",
    "            print(f\"=> old model\")\n",
    "            model_timer = ModelTimer()\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        print(f\"=> model best val: {best_prec1}\")\n",
    "        \n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"=> model epoch: {start_epoch}\")\n",
    "        lr = checkpoint['lr']\n",
    "\n",
    "        print(f\"=> resuming model: {checkpoint['arch']}\")\n",
    "        model = create_model(checkpoint['arch'].split('_')[0], \n",
    "                             int(checkpoint['arch'].split('_')[1]), \n",
    "                             device)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        print(f\"=> resuming optimizer: {checkpoint['opt_name']}\")\n",
    "        bias_decay = True\n",
    "        if checkpoint['bias_decay']:\n",
    "            bias_decay = checkpoint['bias_decay']\n",
    "            \n",
    "        optimizer = get_optimizer(model, checkpoint['opt_name'], bias_decay)\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(ckp_logger_fname, title=model.name, resume=True)\n",
    "        \n",
    "    else:\n",
    "        print(f\"=> restarting training: {ckp_resume}\")\n",
    "        model_timer = ModelTimer()\n",
    "        model = create_model(config.arch, config.pt_layers, device)\n",
    "        optimizer = get_optimizer(model)\n",
    "        logger = Logger(ckp_logger_fname, title=model.name)\n",
    "        logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])\n",
    "              \n",
    "    return best_prec1, model_timer, lr, start_epoch, logger, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inference_model(device, ckp_resume):\n",
    "    if not (ckp_resume and os.path.isfile(ckp_resume)):\n",
    "        print(\"[W] Checkpoint not found for inference.\")\n",
    "        raise \n",
    "    \n",
    "    print(f\"=> loading checkpoint: {ckp_resume}\")\n",
    "    checkpoint = torch.load(ckp_resume)\n",
    "    try:\n",
    "        total_time = checkpoint['total_time']\n",
    "        model_timer = ModelTimer(total_time)\n",
    "        print(f\"=> model trained time: {model_timer}\")\n",
    "    except:\n",
    "        print(f\"=> old model\")\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    print(f\"=> model best val: {best_prec1}\")\n",
    "\n",
    "    print(f\"=> resuming model: {checkpoint['arch']}\")\n",
    "    model = create_model(checkpoint['arch'].split('_')[0], \n",
    "                         int(checkpoint['arch'].split('_')[1]), \n",
    "                         device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "              \n",
    "    return best_prec1, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer):\n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = [AverageMeter() for _ in range(40)]\n",
    "    top1 = [AverageMeter() for _ in range(40)]\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (X, y) in enumerate(tqdm(train_loader, disable=config.disable_tqdm)):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Overlapping transfer if pinned memory\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "    \n",
    "        # compute output\n",
    "        output = model(X)\n",
    "        # measure accuracy and record loss\n",
    "        loss = []\n",
    "        prec1 = []\n",
    "        for j in range(len(output)): \n",
    "            crit = criterion(output[j], y[:, j])\n",
    "            loss.append(crit)\n",
    "            prec1.append(accuracy(output[j], y[:, j], topk=(1,)))\n",
    "            losses[j].update(loss[j].detach().item(), X.size(0))\n",
    "            top1[j].update(prec1[j][0].item(), X.size(0))\n",
    "            \n",
    "        losses_avg = [losses[k].avg for k in range(len(losses))]\n",
    "        top1_avg = [top1[k].avg for k in range(len(top1))]\n",
    "        loss_avg = sum(losses_avg) / len(losses_avg)\n",
    "        prec1_avg = sum(top1_avg) / len(top1_avg)\n",
    "\n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss_sum = sum(loss)\n",
    "        loss_sum.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        print_line = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f}'.format(\n",
    "                        batch=i + 1,\n",
    "                        size=len(train_loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=loss_avg,\n",
    "                        top1=prec1_avg,\n",
    "                        )\n",
    "        if not config.disable_tqdm and (i+1)% 100 == 0:\n",
    "            print(print_line)\n",
    "        bar.suffix  = print_line\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return (loss_avg, prec1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = [AverageMeter() for _ in range(40)]\n",
    "    top1 = [AverageMeter() for _ in range(40)]\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (X, y) in enumerate(tqdm(val_loader, disable=config.disable_tqdm)):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # Overlapping transfer if pinned memory\n",
    "            X = X.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(X)\n",
    "            # measure accuracy and record loss\n",
    "            loss = []\n",
    "            prec1 = []\n",
    "            for j in range(len(output)):\n",
    "                loss.append(criterion(output[j], y[:, j]))\n",
    "                prec1.append(accuracy(output[j], y[:, j], topk=(1,)))\n",
    "                \n",
    "                losses[j].update(loss[j].detach().item(), X.size(0))\n",
    "                top1[j].update(prec1[j][0].item(), X.size(0))\n",
    "            losses_avg = [losses[k].avg for k in range(len(losses))]\n",
    "            top1_avg = [top1[k].avg for k in range(len(top1))]\n",
    "            loss_avg = sum(losses_avg) / len(losses_avg)\n",
    "            prec1_avg = sum(top1_avg) / len(top1_avg)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # plot progress\n",
    "            print_line = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f}'.format(\n",
    "                            batch=i + 1,\n",
    "                            size=len(val_loader),\n",
    "                            data=data_time.avg,\n",
    "                            bt=batch_time.avg,\n",
    "                            total=bar.elapsed_td,\n",
    "                            eta=bar.eta_td,\n",
    "                            loss=loss_avg,\n",
    "                            top1=prec1_avg,\n",
    "                            )\n",
    "\n",
    "            bar.suffix  = print_line\n",
    "            bar.next()  \n",
    "\n",
    "    if not config.disable_tqdm:\n",
    "        print(print_line)        \n",
    "    bar.finish()\n",
    "    return (loss_avg, prec1_avg, top1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataloaders, model, criterion, optimizer, logger, start_epoch, best_prec1, run_name, model_timer):\n",
    "    # visualization\n",
    "    writer = SummaryWriter(os.path.join(config.tensorboard_dir, run_name))\n",
    "    \n",
    "    scheduler = get_scheduler(optimizer, len(dataloaders['train']), config.epochs-start_epoch)\n",
    "    \n",
    "    stagnant_val_loss_ctr = 0\n",
    "    min_val_loss = 1.\n",
    "    \n",
    "    for epoch in range(start_epoch, config.epochs):\n",
    "        model_timer.start_epoch_timer()\n",
    "        if not scheduler:\n",
    "            lr = adjust_learning_rate(optimizer, config.lr_decay, epoch, gamma=config.gamma, step=config.step,\n",
    "                                     total_epochs=config.epochs, turning_point=config.turning_point,\n",
    "                                     schedule=config.schedule)\n",
    "        else:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print('\\nEpoch: [%d | %d] LR: %.16f' % (epoch + 1, config.epochs, lr))\n",
    "\n",
    "        # train for one epoch\n",
    "        train_loss, train_acc = train(dataloaders['train'], model, criterion, optimizer)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss, prec1, _ = validate(dataloaders['val'], model, criterion)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(None if config.scheduler != 'ReduceLROnPlateau' else val_loss)\n",
    "            \n",
    "        # append logger file\n",
    "        logger.append([lr, train_loss, val_loss, train_acc, prec1])\n",
    "\n",
    "        # tensorboardX\n",
    "        writer.add_scalar('learning rate', lr, epoch + 1)\n",
    "        writer.add_scalars('loss', {'train loss': train_loss, 'validation loss': val_loss}, epoch + 1)\n",
    "        writer.add_scalars('accuracy', {'train accuracy': train_acc, 'validation accuracy': prec1}, epoch + 1)\n",
    "\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        model_timer.stop_epoch_timer()\n",
    "        model.save_ckp({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': model.name,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'opt_name': config.optimizer,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'lr': lr,\n",
    "            'total_time': model_timer.total_time,\n",
    "            'bias_decay': config.no_bias_bn_decay,\n",
    "        }, is_best, config.checkpoint_fname,config.bestmodel_fname)\n",
    "        \n",
    "        if config.early_stopping:\n",
    "            if val_loss >= min_val_loss:\n",
    "                stagnant_val_loss_ctr += 1\n",
    "                if stagnant_val_loss_ctr > config.es_min and stagnant_val_loss_ctr >= config.es_patience: \n",
    "                    break\n",
    "            else:\n",
    "                stagnant_val_loss_ctr = 0\n",
    "                min_val_loss = val_loss\n",
    "\n",
    "    logger.close()\n",
    "    logger.plot()\n",
    "    save_path = None\n",
    "    if config.train_saveplot:\n",
    "        save_path = os.path.join(config.CHECKPOINT_DIR, \"losses.jpg\")\n",
    "    logger.plot_special(save_path)\n",
    "    savefig(config.train_plotfig)\n",
    "    writer.close()\n",
    "\n",
    "    print('Best accuracy:')\n",
    "    print(best_prec1)\n",
    "    return model_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_name_time(model, criterion, optimizer, comments):\n",
    "    try:\n",
    "        if criterion.name:\n",
    "            p_criterion = criterion.name\n",
    "    except:\n",
    "        p_criterion = 'CE'\n",
    "\n",
    "    p_optimizer = f'{str(optimizer).split(\"(\")[0].strip()}'\n",
    "    p_scheduler = f'lr{config.lr}_wd{config.weight_decay}'\n",
    "    if config.scheduler == 'Manual':\n",
    "        p_scheduler += f'_{config.lr_decay}'\n",
    "        if config.lr_decay == 'step':\n",
    "            p_scheduler += f'_g{config.gamma}_sp{config.step}'\n",
    "        elif config.lr_decay == 'linear2exp':\n",
    "            p_scheduler += f'_g{config.gamma}_tp{config.turning_point}'\n",
    "        elif config.lr_decay == 'schedule':\n",
    "            p_scheduler += f'_g{config.gamma}_sch{config.schedule}'\n",
    "    else: \n",
    "        p_scheduler += f'_{config.scheduler}'\n",
    "    \n",
    "    run_name = f'{model.name}_{config.manual_seed}_s{start_epoch}e{config.epochs}_' \\\n",
    "                + f'tb{config.train_batch}_vb{config.test_batch}_' \\\n",
    "                + f'{p_criterion}_{p_optimizer}_' \\\n",
    "                + f'{comments}_' \\\n",
    "                + f'{p_scheduler}'\n",
    "    \n",
    "    run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(run_name, run_time)\n",
    "    return run_name, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PWCHnDeVD4WT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: {'test': 19962}\n",
      "Class Labels: 40\n",
      "=> Training model: False\n",
      "=> loading checkpoint: checkpoints\\model_best.pth.tar\n",
      "=> model trained time: 00:25:39\n",
      "=> model best val: 91.87018673975948\n",
      "=> resuming model: FaceAttrResNet_18\n",
      "=> creating model 'FaceAttrResNet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:28<00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156/156) Data: 0.049s | Batch: 0.178s | Total: 0:00:27 | ETA: 0:00:01 | Loss: 0.0307 | top1:  88.4389\n",
      "=> Best test accuracy: 88.43890891137718, Model val acc: 91.87018673975948\n",
      "5_o_Clock_Shadow: 92.81635104485999\n",
      "Arched_Eyebrows: 74.88227632043119\n",
      "Attractive: 81.92565875315688\n",
      "Bags_Under_Eyes: 80.5780983762337\n",
      "Bald: 99.0532010820559\n",
      "Bangs: 95.090672252102\n",
      "Big_Lips: 68.39495039430508\n",
      "Big_Nose: 82.51177233233622\n",
      "Black_Hair: 89.41989779589399\n",
      "Blond_Hair: 95.7018334522788\n",
      "Blurry: 96.03747116087338\n",
      "Brown_Hair: 87.81685202418653\n",
      "Bushy_Eyebrows: 90.51698225025375\n",
      "Chubby: 95.2910529754764\n",
      "Double_Chin: 95.72187151850112\n",
      "Eyeglasses: 99.1884580733912\n",
      "Goatee: 96.7939084099572\n",
      "Gray_Hair: 98.14647827438907\n",
      "Heavy_Makeup: 89.62528804346789\n",
      "High_Cheekbones: 79.15038570002365\n",
      "Male: 96.48331827649659\n",
      "Mouth_Slightly_Open: 65.71485822949163\n",
      "Mustache: 96.55345152662007\n",
      "Narrow_Eyes: 86.08355872342754\n",
      "No_Beard: 94.3542730875859\n",
      "Oval_Face: 71.24035665704903\n",
      "Pale_Skin: 96.50836586386082\n",
      "Pointy_Nose: 74.95240956443952\n",
      "Receding_Hairline: 92.14507564295869\n",
      "Rosy_Cheeks: 93.04678888897084\n",
      "Sideburns: 97.58541228639591\n",
      "Smiling: 75.18284741772307\n",
      "Straight_Hair: 83.02274321745374\n",
      "Wavy_Hair: 82.27632498542886\n",
      "Wearing_Earrings: 89.46999298285272\n",
      "Wearing_Hat: 98.9880773500168\n",
      "Wearing_Lipstick: 88.93397455394131\n",
      "Wearing_Necklace: 86.21380619362091\n",
      "Wearing_Necktie: 93.00170322621159\n",
      "Young: 87.13555754636813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mtimer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-23d3bd3abad2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_prec1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"=> Model trained time: {mtimer}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mtimer' is not defined"
     ]
    }
   ],
   "source": [
    "# config.epoch = 1\n",
    "#model = create_model(device)\n",
    "dataloaders, attribute_names = load_dataloaders()\n",
    "criterion = get_criterion()\n",
    "#optimizer = get_optimizer(model)\n",
    "\n",
    "print(f\"=> Training model: {not config.evaluate}\")\n",
    "if config.evaluate:\n",
    "    best_prec1, model = load_inference_model(device, config.bestmodel_fname) # checkpoint_fname bestmodel_fname\n",
    "    test_loss, prec1, top1 = validate(dataloaders['test'], model, criterion)\n",
    "    print(f\"=> Best test accuracy: {prec1}, Model val acc: {best_prec1}\")\n",
    "    print_attribute_acc(top1, attribute_names)\n",
    "else:\n",
    "    best_prec1, model_timer, lr, start_epoch, logger, model, optimizer = resume_checkpoint(device, config.ckp_logger_fname, config.ckp_resume)\n",
    "    run_name, run_time = get_run_name_time(model, criterion, optimizer, comments)\n",
    "    mtimer = trainer(dataloaders, model, criterion, optimizer, logger, start_epoch, best_prec1, run_name, model_timer)\n",
    "    print(f\"=> Model trained time: {mtimer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.evaluate:\n",
    "    config.evaluate = True\n",
    "    #model = create_model(device)\n",
    "    dataloaders, attribute_names = load_dataloaders()\n",
    "    criterion = get_criterion()\n",
    "    #optimizer = get_optimizer(model)\n",
    "    \n",
    "    best_prec1, model = load_inference_model(device, config.bestmodel_fname) # checkpoint_fname bestmodel_fname\n",
    "    #best_prec1, mtimer, _, _, logger, = resume_checkpoint(model, optimizer, config.ckp_logger_fname, config.checkpoint_fname)\n",
    "    test_loss, prec1, top1 = validate(dataloaders['test'], model, criterion)\n",
    "    print(f\"=> Best test accuracy: {prec1}, Model val acc: {best_prec1}\")\n",
    "    print_attribute_acc(top1, attribute_names)\n",
    "#     best_prec1, mtimer, _, _, _, = resume_checkpoint(model, optimizer, config.ckp_logger_fname, config.bestmodel_fname)# config.bestmodel_fname  config.checkpoint_fname\n",
    "#     #print(model)\n",
    "#     test_loss, prec1, top1 = validate(dataloaders['test'], model, criterion)\n",
    "#     print(f\"=> Best test accuracy: {prec1}, Model val acc: {best_prec1}\")\n",
    "#     print_attribute_acc(top1, attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISJUPYTER:\n",
    "    # Wait for notebook to save\n",
    "    %autosave 1\n",
    "    time.sleep(150)\n",
    "    %autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_everything(run_time, run_name, title, backup_nb=ISJUPYTER):\n",
    "    # backup checkpoints\n",
    "    print(f\"=> backing up checkpoints... \")\n",
    "    run_dir = os.path.join(config.BACKUP_DIR, run_name, run_time)\n",
    "    create_dir_ifne(run_dir)\n",
    "    fromDirectory = config.CHECKPOINT_DIR\n",
    "    toDirectory = run_dir\n",
    "    copy_tree(fromDirectory, toDirectory)\n",
    "    \n",
    "    if backup_nb:\n",
    "        print(f\"=> backing up notebook... \")\n",
    "        # backup notebook html\n",
    "        nb_name = title + '.ipynb'\n",
    "        html_name = title + '.html'\n",
    "        save_name = os.path.join(run_dir, html_name)\n",
    "        !jupyter nbconvert --to html $nb_name\n",
    "        shutil.move(html_name, save_name)\n",
    "    \n",
    "backup_everything(run_time, run_name, title, backup_nb=ISJUPYTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.auto_hibernate and False:\n",
    "    os.system('shutdown -h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOYWQroywTLOsSvW/4lPmBg",
   "collapsed_sections": [],
   "name": "ai6126-project1-colab-v0.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
