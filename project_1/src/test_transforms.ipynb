{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import config\n",
    "from celeba_dataset import CelebaDataset\n",
    "\n",
    "# set the backend of matplotlib to the 'inline' backend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "def seed_everything(seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000) # create random seed\n",
    "        print(f'random seed used: {seed}')\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=config.manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation and testing\n",
    "\n",
    "def load_dataloader(print_info=True, albu_transforms = True):\n",
    "    if config.evaluate:\n",
    "        phases = ['test']\n",
    "    else:\n",
    "        phases = ['train', 'val']\n",
    "\n",
    "    attribute_names = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', \n",
    "                       'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', \n",
    "                       'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
    "                       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', \n",
    "                       'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
    "                       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', \n",
    "                       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
    "    \n",
    "    attributes_list = {\n",
    "        'train': config.TRAIN_ATTRIBUTE_LIST,\n",
    "        'val': config.VAL_ATTRIBUTE_LIST,\n",
    "        'test': config.TEST_ATTRIBUTE_LIST\n",
    "    }\n",
    "\n",
    "    batch_sizes = {\n",
    "        'train': config.train_batch,\n",
    "        'val': config.test_batch,\n",
    "        'test': config.test_batch\n",
    "    }\n",
    "    if not albu_transforms:\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize(128), #new\n",
    "                transforms.CenterCrop(128), #new\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=10), #new\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "                transforms.RandomErasing()\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(128), #new\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(128), #new\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        }\n",
    "    else:\n",
    "        normalize_A = A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                  std=(0.229, 0.224, 0.225))\n",
    "        data_transforms = {\n",
    "            'train': A.Compose([\n",
    "                #A.RandomResizedCrop(148, 148), # cuts out too much attributes, use centercrop instead\n",
    "                A.CenterCrop(height=168, width=168),\n",
    "                #A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, \n",
    "                #                  rotate_limit=20, p=0.5), # AFFACT https://arxiv.org/pdf/1611.06158.pdf\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=14, sat_shift_limit=14, val_shift_limit=14, p=0.5),\n",
    "                A.FancyPCA(alpha=0.1, p=0.5), #http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "                #A.GaussianBlur(p=0.1), # AFFACT https://arxiv.org/pdf/1611.06158.pdf\n",
    "                A.GaussNoise(var_limit=10.0, p=0.5), \n",
    "                #A.CoarseDropout(max_holes=1, max_height=74, max_width=74, \n",
    "                #                min_height=49, min_width=49, fill_value=0, p=0.2), #https://arxiv.org/pdf/1708.04896.pdf\n",
    "                normalize_A,\n",
    "                ToTensorV2(),\n",
    "                \n",
    "            ]),\n",
    "            'val': A.Compose([\n",
    "                #Rescale an image so that minimum side is equal to max_size 178 (shortest edge of celeba)\n",
    "                A.SmallestMaxSize(max_size=178), \n",
    "                A.CenterCrop(height=168, width=168),\n",
    "                normalize_A,\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "            'test': A.Compose([\n",
    "                A.SmallestMaxSize(max_size=178),\n",
    "                A.CenterCrop(height=168, width=168),\n",
    "                normalize_A,\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        }\n",
    "\n",
    "\n",
    "    image_datasets = {x: CelebaDataset(config.IMG_DIR, attributes_list[x], \n",
    "                                       data_transforms[x]) \n",
    "                      for x in phases}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                  batch_size=batch_sizes[x],\n",
    "                                                  pin_memory=True, shuffle=(x == 'train'), \n",
    "                                                  num_workers=config.dl_workers) \n",
    "                   for x in phases}\n",
    "    if print_info:\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in phases}\n",
    "        print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "        \n",
    "    if config.evaluate:\n",
    "        class_names = image_datasets['test'].targets\n",
    "    else:\n",
    "        class_names = image_datasets['train'].targets\n",
    "        \n",
    "    print(f\"Class Labels: {len(class_names[0])}\")\n",
    "    assert len(attribute_names) == len(class_names[0])\n",
    "    return image_datasets, dataloaders, attribute_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: {'train': 162770, 'val': 19867}\n",
      "Class Labels: 40\n"
     ]
    }
   ],
   "source": [
    "albu_transforms = True\n",
    "image_datasets, dataloaders, attr_names = load_dataloader(albu_transforms = albu_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 168, 168) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-55171e48b7a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0malbu_transforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mvisualize_augmentations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mvisualize_augmentations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-55171e48b7a2>\u001b[0m in \u001b[0;36mvisualize_augmentations\u001b[1;34m(dataset, idx, samples, cols)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai6126acv_p1\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai6126acv_p1\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai6126acv_p1\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    703\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    704\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 705\u001b[1;33m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[0;32m    706\u001b[0m                             .format(self._A.shape))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 168, 168) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()   \n",
    "\n",
    "if albu_transforms:\n",
    "    visualize_augmentations(image_datasets['train'], idx=44)\n",
    "    visualize_augmentations(image_datasets['val'], idx=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decide which device we want to run on\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "real_batch = next(iter(dataloaders['train']))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before V9.1\n",
    "#                 A.SmallestMaxSize(max_size=160),\n",
    "#                 A.HorizontalFlip(p=0.5),\n",
    "#                 A.MotionBlur(p=0.1),\n",
    "#                 A.GaussNoise(p=0.5),\n",
    "#                 A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, \n",
    "#                                    rotate_limit=15, p=0.5),\n",
    "#                 A.RandomBrightnessContrast(p=0.5),\n",
    "#                 #A.CoarseDropout(),\n",
    "#                 normalize_A,\n",
    "#                 ToTensorV2(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V9.1\n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "        A.SmallestMaxSize(max_size=160),\n",
    "        #A.RandomCrop(width=148, height=148),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, \n",
    "                          rotate_limit=15, p=0.5),\n",
    "        A.FancyPCA(p=0.5), #http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "        A.GaussNoise(var_limit=(10.0, 80.0), p=0.5),\n",
    "        A.GaussianBlur(p=0.2),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        #A.CoarseDropout(),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    'val': A.Compose([\n",
    "        A.SmallestMaxSize(max_size=160),\n",
    "        #A.CenterCrop(height=148, width=148),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        A.SmallestMaxSize(max_size=160),\n",
    "        #A.CenterCrop(height=148, width=148),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1.1\n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "        #A.RandomResizedCrop(148, 148), # cuts out too much attributes, use centercrop instead\n",
    "        A.CenterCrop(height=148, width=148),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, \n",
    "                          rotate_limit=15, p=0.5), # not in bags of tricks\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=14, sat_shift_limit=14, val_shift_limit=14, p=0.5),\n",
    "        A.FancyPCA(alpha=0.1, p=0.5), #http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "        A.GaussNoise(var_limit=10.0, p=0.5), \n",
    "        A.CoarseDropout(max_holes=1, max_height=74, max_width=74, \n",
    "                        min_height=49, min_width=49, fill_value=0, p=0.2), #https://arxiv.org/pdf/1708.04896.pdf\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "\n",
    "    ]),\n",
    "    'val': A.Compose([\n",
    "        #Rescale an image so that minimum side is equal to max_size 178 (shortest edge of celeba)\n",
    "        A.SmallestMaxSize(max_size=178), \n",
    "        A.CenterCrop(height=148, width=148),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        A.SmallestMaxSize(max_size=178),\n",
    "        A.CenterCrop(height=148, width=148),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1.2\n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "        #A.RandomResizedCrop(148, 148), # cuts out too much attributes, use centercrop instead\n",
    "        A.CenterCrop(height=168, width=168),\n",
    "        #A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, \n",
    "        #                  rotate_limit=20, p=0.5), # AFFACT https://arxiv.org/pdf/1611.06158.pdf\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=14, sat_shift_limit=14, val_shift_limit=14, p=0.5),\n",
    "        A.FancyPCA(alpha=0.1, p=0.5), #http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "        #A.GaussianBlur(p=0.1), # AFFACT https://arxiv.org/pdf/1611.06158.pdf\n",
    "        A.GaussNoise(var_limit=10.0, p=0.5), \n",
    "        #A.CoarseDropout(max_holes=1, max_height=74, max_width=74, \n",
    "        #                min_height=49, min_width=49, fill_value=0, p=0.2), #https://arxiv.org/pdf/1708.04896.pdf\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "\n",
    "    ]),\n",
    "    'val': A.Compose([\n",
    "        #Rescale an image so that minimum side is equal to max_size 178 (shortest edge of celeba)\n",
    "        A.SmallestMaxSize(max_size=178), \n",
    "        A.CenterCrop(height=168, width=168),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        A.SmallestMaxSize(max_size=178),\n",
    "        A.CenterCrop(height=168, width=168),\n",
    "        normalize_A,\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
